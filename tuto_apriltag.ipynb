{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AprilTag system: 6DoF vision with monocular cameras\n",
    "\n",
    "\n",
    "The library Apriltag allows us to detect markers in images, and to compute the relative pose between the camera coordinate frame and the tag.\n",
    "\n",
    "We will need Apriltag, OpenCV-python, and some NumPy operations to proceed. Install them with\n",
    "\n",
    "```terminal\n",
    "python -m pip install apriltag\n",
    "python -m pip install opencv-python\n",
    "```\n",
    "Then import them to your python project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apriltag\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting apriltags in images\n",
    "\n",
    "First we need to read an image into memory, and store it as greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('./data/skew.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.imshow(\"Image view\", image)\n",
    "cv2.waitKey(1000) # waits until a key is pressed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need an apriltag detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = apriltag.Detector()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can process the image, and see what comes out of the Apriltag detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detections = \n",
      " [Detection(tag_family=b'tag36h11', tag_id=5, hamming=0, goodness=0.0, decision_margin=87.4375, homography=array([[ 9.28986229e-01,  2.25901040e-01,  2.52270766e+00],\n",
      "       [-1.91712313e-01,  2.44261297e-01,  1.57486813e+00],\n",
      "       [ 1.30679241e-03, -1.56725245e-03,  1.87352580e-02]]), center=array([134.65027603,  84.05905775]), corners=array([[ 72.0067749 ,  80.14012146],\n",
      "       [149.27796936,  52.70389938],\n",
      "       [199.06008911,  88.08849335],\n",
      "       [114.72151947, 126.77729797]]))]\n"
     ]
    }
   ],
   "source": [
    "detections = detector.detect(image)\n",
    "\n",
    "print('detections = \\n', detections)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've processed an image, and discovered one Tag, labelled with the ID=5.\n",
    "\n",
    "We have some more information regarding this detection, which we can access easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag # 5\n",
      "detection hamming 0\n",
      "detection goodness 0.0\n",
      "detection margin 87.4375\n",
      "homography\n",
      " [[ 9.28986229e-01  2.25901040e-01  2.52270766e+00]\n",
      " [-1.91712313e-01  2.44261297e-01  1.57486813e+00]\n",
      " [ 1.30679241e-03 -1.56725245e-03  1.87352580e-02]]\n",
      "tag center [134.65027603  84.05905775]\n",
      "tag corners\n",
      " [[ 72.0067749   80.14012146]\n",
      " [149.27796936  52.70389938]\n",
      " [199.06008911  88.08849335]\n",
      " [114.72151947 126.77729797]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for detection in detections:  \n",
    "\n",
    "    print('tag #',              detection.tag_id)\n",
    "    print('detection hamming',  detection.hamming)\n",
    "    print('detection goodness', detection.goodness)\n",
    "    print('detection margin',   detection.decision_margin)\n",
    "    print('homography\\n',       detection.homography)\n",
    "    print('tag center',         detection.center)\n",
    "    print('tag corners\\n',      detection.corners)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These fields are documented in Apriltag as follows\n",
    "\n",
    "    # The decoded ID of the tag\n",
    "    tag_id\n",
    "\n",
    "    # How many error bits were corrected? Note: accepting large numbers of\n",
    "    # corrected errors leads to greatly increased false positive rates.\n",
    "    # NOTE: As of this implementation, the detector cannot detect tags with\n",
    "    # a hamming distance greater than 2.\n",
    "    hamming\n",
    "\n",
    "    # A measure of the quality of tag localization: measures the\n",
    "    # average contrast of the pixels around the border of the\n",
    "    # tag. refine_pose must be enabled, or this field will be zero.\n",
    "    goodness\n",
    "\n",
    "    # A measure of the quality of the binary decoding process: the\n",
    "    # average difference between the intensity of a data bit versus\n",
    "    # the decision threshold. Higher numbers roughly indicate better\n",
    "    # decodes. This is a reasonable measure of detection accuracy\n",
    "    # only for very small tags-- not effective for larger tags (where\n",
    "    # we could have sampled anywhere within a bit cell and still\n",
    "    # gotten a good detection.)\n",
    "    decision_margin\n",
    "\n",
    "    # The 3x3 homography matrix describing the projection from an\n",
    "    # \"ideal\" tag (with corners at (-1,-1), (1,-1), (1,1), and (-1,\n",
    "    # 1)) to pixels in the image. This matrix will be freed by\n",
    "    # apriltag_detection_destroy.\n",
    "    homography;\n",
    "\n",
    "    # The center of the detection in image pixel coordinates.\n",
    "    center;\n",
    "\n",
    "    # The corners of the tag in image pixel coordinates. These always\n",
    "    # wrap counter-clock wise around the tag.\n",
    "    corners;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing camera-to-tag relative pose\n",
    "\n",
    "Camera-to-tag transforms can be obtained from the detected corners if we know\n",
    "  - the geometry of the tags\n",
    "  - the geometry of the camera (intrinsic and distortion parameters)\n",
    "\n",
    "So let us define these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag geometry\n",
    "tag_size    = 0.2\n",
    "tag_corners = tag_size / 2 * np.array([[-1,1,0],[1,1,0],[1,-1,0],[-1,-1,0]])\n",
    "\n",
    "# Camera calibration\n",
    "K           = np.array([[   419.53, 0.0,    427.88  ], \n",
    "                        [   0.0,    419.53, 241.32  ], \n",
    "                        [   0.0,    0.0,    1.0     ]])  \n",
    "# warning: these params do not corresopnd to the ones of the camera used to take the image skew.jpeg\n",
    "\n",
    "distortion_model = np.array([])  # we assume rectified images, therefore with no distortion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here there are two ways of computing the relative pose between the camera and the tag.\n",
    "\n",
    "- One method uses the homography provided by the detector to extract translation `T` and rotation `R`. This method is unstable and we do not recommend it.\n",
    "\n",
    "- The other method uses the PnP algorithm which, given the four corners of the tag in tag reference (which are known), the projected same corners in the image obtained by the detector, and the camera calibration parameters, computes the transformation (`T`,`R`) between camera and tag. \n",
    "\n",
    "We use OpenCV for this, and recover a translation vector and a rotation vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = \n",
      " [[-0.71469273]\n",
      " [-0.38025099]\n",
      " [ 1.02429824]]\n",
      "w = \n",
      " [[ 2.39555008]\n",
      " [-1.05666679]\n",
      " [ 0.51011416]]\n"
     ]
    }
   ],
   "source": [
    "(_, rotation_vector, translation_vector) = cv2.solvePnP(tag_corners, detection.corners, K, distortion_model, flags = cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "\n",
    "T = translation_vector\n",
    "w = rotation_vector\n",
    "\n",
    "print('T = \\n', T)\n",
    "print('w = \\n', w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that although `T` and `w` are vectors, they come represented as 2-dimensional arrays. We can clean them up to avoid trouble down the road:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = \n",
      " [-0.71469273 -0.38025099  1.02429824]\n",
      "w = \n",
      " [ 2.39555008 -1.05666679  0.51011416]\n"
     ]
    }
   ],
   "source": [
    "T = (T.T)[0]\n",
    "w = (w.T)[0]\n",
    "\n",
    "print('T = \\n', T)\n",
    "print('w = \\n', w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now important to know how to interpret these T and R. Are they camera-to-tag? Or tag-to-camera? Reading the doc of cv2.computePnP(), we see that they transform points in tag frame into points in camera frame. Let us rename the variables to account for this and be more verbose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_c_t = T\n",
    "w_c_t = w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a rotation matrix from a rotation vector, we require the exponential in SO(3), or the Rodrigues formula. We use pinocchio for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = \n",
      " [[ 0.63436322 -0.75956485  0.14368208]\n",
      " [-0.58494682 -0.59316663 -0.55316414]\n",
      " [ 0.50539145  0.26686061 -0.82058814]]\n"
     ]
    }
   ],
   "source": [
    "import pinocchio as pin\n",
    "\n",
    "R_c_t = pin.exp(w_c_t)\n",
    "\n",
    "print('R = \\n',R_c_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reproject the tag corners into the image, and see if they differ much from the detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected corners\n",
      " [[[ 69.72493078  81.51959134]]\n",
      "\n",
      " [[150.91120302  51.62626837]]\n",
      "\n",
      " [[197.61212802  89.45084717]]\n",
      "\n",
      " [[116.83486974 125.06570759]]]\n",
      "\n",
      "detected corners\n",
      " [[ 72.0067749   80.14012146]\n",
      " [149.27796936  52.70389938]\n",
      " [199.06008911  88.08849335]\n",
      " [114.72151947 126.77729797]]\n"
     ]
    }
   ],
   "source": [
    "projected_corners = cv2.projectPoints(tag_corners, R_c_t, T_c_t, K, distortion_model)\n",
    "\n",
    "print('projected corners\\n', projected_corners[0])\n",
    "print()\n",
    "print('detected corners\\n', detection.corners)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they kind of match, but not really. This is because the image used was taken from the internet, and we do not have the correct camera calibration parameters.\n",
    "\n",
    "Let us then re-do the whole process with a proper image taken by the camera whose parameters we know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag #  11\n",
      "projected corners\n",
      " [[[530.33061847 324.85984592]]\n",
      "\n",
      " [[562.64324848 372.42973975]]\n",
      "\n",
      " [[513.0002665  382.6174256 ]]\n",
      "\n",
      " [[483.49205875 331.01970981]]]\n",
      "\n",
      "detected corners\n",
      " [[530.2746582  324.92544556]\n",
      " [562.69659424 372.34301758]\n",
      " [512.9286499  382.71353149]\n",
      " [483.56466675 330.94699097]]\n",
      "Tag #  23\n",
      "projected corners\n",
      " [[[101.27210258 214.14114391]]\n",
      "\n",
      " [[175.20315405 222.45660138]]\n",
      "\n",
      " [[155.42740652 264.68101411]]\n",
      "\n",
      " [[ 70.584094   257.57751718]]]\n",
      "\n",
      "detected corners\n",
      " [[101.36871338 214.23381042]\n",
      " [175.09712219 222.36929321]\n",
      " [155.53752136 264.78646851]\n",
      " [ 70.48442841 257.46499634]]\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('./data/visual_odom_laas_corridor/short2/frame0000.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow(\"Image view\", image)\n",
    "cv2.waitKey(10) # waits so the image can be drawn (??)\n",
    "\n",
    "detections = detector.detect(image)\n",
    "\n",
    "for detection in detections[0:2]:   # we'll show results of only 2 detections\n",
    "\n",
    "    print('Tag # ', detection.tag_id)\n",
    "\n",
    "    (_, w_c_t, T_c_t) = cv2.solvePnP(tag_corners, detection.corners, K, distortion_model, flags = cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "    R_c_t = pin.exp(w_c_t)\n",
    "\n",
    "    projected_corners = cv2.projectPoints(tag_corners, R_c_t, T_c_t, K, distortion_model)\n",
    "\n",
    "    print('projected corners\\n', projected_corners[0])\n",
    "    print()\n",
    "    print('detected corners\\n', detection.corners)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing tag detection quality\n",
    "\n",
    "The quality of a detection can be assessed with different metrics. We provide here some clues:\n",
    "\n",
    "- Use the `detection.goodness` result\n",
    "- Use the `detection.hamming` result\n",
    "- Use the `detection.detection_margin` result\n",
    "- Use the reprojection error of the corners. That is, the norm of the difference between `projected_corners` and `detected_corners` above. \n",
    "\n",
    "We do not provide further insight here, but you may explore these possibilities should your SLAM algorithm show signs of fragility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
